{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "composite-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import wordcloud\n",
    "import sklearn\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adjusted-planet",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "internal-breeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(Path(\"../../SubredditData\") / \"mbti_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adolescent-working",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8670</th>\n",
       "      <td>ISFP</td>\n",
       "      <td>'https://www.youtube.com/watch?v=t8edHB_h908||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8671</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>'So...if this thread already exists someplace ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8672</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'So many questions when i do these things.  I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8673</th>\n",
       "      <td>INFP</td>\n",
       "      <td>'I am very conflicted right now when it comes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8674</th>\n",
       "      <td>INFP</td>\n",
       "      <td>'It has been too long since I have been on per...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8675 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                              posts\n",
       "0     INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1     ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2     INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3     INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4     ENTJ  'You're fired.|||That's another silly misconce...\n",
       "...    ...                                                ...\n",
       "8670  ISFP  'https://www.youtube.com/watch?v=t8edHB_h908||...\n",
       "8671  ENFP  'So...if this thread already exists someplace ...\n",
       "8672  INTP  'So many questions when i do these things.  I ...\n",
       "8673  INFP  'I am very conflicted right now when it comes ...\n",
       "8674  INFP  'It has been too long since I have been on per...\n",
       "\n",
       "[8675 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "coordinated-ireland",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_stopwords = stopwords.words('english') + ['INFJ', 'INFP', 'INTP', 'INTJ', 'ISFJ', 'ISFP', 'ISTP', 'ISTJ', 'ENFJ',\n",
    "                                               'ENFP', 'ENTJ', 'ENTP', 'ESFJ', 'ESFP', 'ESTJ', 'ESTP',\n",
    "                                               'people', 'People', 'think', 'Think', 'know', 'Know', \"I'm\",\n",
    "                                               'like', 'would', 'Would', 'really', 'Really', 'get', 'Get']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "weird-diamond",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiwarriors/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['enfj', 'enfp', 'entj', 'entp', 'esfj', 'esfp', 'estj', 'estp', 'infj', 'infp', 'intj', 'intp', 'isfj', 'isfp', 'istj', 'istp'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=full_stopwords)\n",
    "tfidf= TfidfVectorizer(stop_words=full_stopwords)\n",
    "\n",
    "\n",
    "v_tf = vectorizer.fit_transform(df.posts)\n",
    "tf_tf = tfidf.fit_transform(df.posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-monday",
   "metadata": {},
   "source": [
    "Now, let's check the word frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-pathology",
   "metadata": {},
   "source": [
    "Now to make a list for every personality type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "parental-cancer",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_infj_posts = df[ df['type'] == 'INFJ' ]['posts']\n",
    "\n",
    "df_intp_posts = df[df['type'] == 'INTP']['posts']\n",
    "df_intj_posts = df[df['type'] == 'INTJ']['posts']\n",
    "df_isfp_posts = df[df['type'] == 'ISFP']['posts']\n",
    "df_isfj_posts = df[df['type'] == 'ISFJ']['posts']\n",
    "df_istp_posts = df[df['type'] == 'ISTP']['posts']\n",
    "df_istj_posts = df[df['type'] == 'ISTJ']['posts']\n",
    "df_enfp_posts = df[df['type'] == 'ENFP']['posts']\n",
    "df_enfj_posts = df[df['type'] == 'ENFJ']['posts']\n",
    "df_entp_posts = df[df['type'] == 'ENTP']['posts']\n",
    "df_entj_posts = df[df['type'] == 'ENTJ']['posts']\n",
    "df_esfj_posts = df[df['type'] == 'ESFJ']['posts']\n",
    "df_esfp_posts = df[df['type'] == 'ESFP']['posts']\n",
    "df_estj_posts = df[df['type'] == 'ESTJ']['posts']\n",
    "df_estp_posts = df[df['type'] == 'ESTP']['posts']\n",
    "df_infp_posts = df[df['type'] == 'INFP']['posts']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-electric",
   "metadata": {},
   "source": [
    "For every personality type, we must put all of the posts in one single array and then convert that array into a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "noticed-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INFJ\n",
    "infj_posts = [] \n",
    "infj_text = \"\"\n",
    "for post_cluster in df_infj_posts:\n",
    "    infj_posts += post_cluster.split('|||')\n",
    "for text in infj_posts:\n",
    "    infj_text += \"\".join(text)+\" \"\n",
    "    \n",
    "#INTP\n",
    "intp_posts = []\n",
    "intp_text = \"\"\n",
    "for post_cluster in df_intp_posts:\n",
    "    intp_posts += post_cluster.split('|||')\n",
    "for text in intp_posts:\n",
    "    intp_text += \"\".join(text)+\" \"\n",
    "    \n",
    "#INTJ\n",
    "intj_posts = []\n",
    "intj_text = \"\"\n",
    "for post_cluster in df_intj_posts:\n",
    "    intj_posts += post_cluster.split('|||')\n",
    "for text in intj_posts:\n",
    "    intj_text += \"\".join(text)+\" \"\n",
    "\n",
    "#ISFP\n",
    "isfp_posts = []\n",
    "isfp_text = \"\"\n",
    "for post_cluster in df_isfp_posts:\n",
    "    isfp_posts += post_cluster.split('|||')\n",
    "for text in isfp_posts:\n",
    "    isfp_text += \"\".join(text)+\" \"\n",
    "    \n",
    "#ISFJ\n",
    "isfj_posts = []\n",
    "isfj_text = \"\"\n",
    "for post_cluster in df_isfj_posts:\n",
    "    isfj_posts += post_cluster.split('|||')\n",
    "for text in isfj_posts:\n",
    "    isfj_text += \"\".join(text)+\" \"\n",
    "\n",
    "#ISTP\n",
    "istp_posts = []\n",
    "istp_text = \"\"\n",
    "for post_cluster in df_istp_posts:\n",
    "    istp_posts += post_cluster.split('|||')\n",
    "for text in istp_posts:\n",
    "    istp_text += \"\".join(text)+\" \"\n",
    "\n",
    "#ISTJ\n",
    "istj_posts = []\n",
    "istj_text = \"\"\n",
    "for post_cluster in df_istj_posts:\n",
    "    istj_posts += post_cluster.split('|||')\n",
    "for text in istj_posts:\n",
    "    istj_text += \"\".join(text)+\" \"\n",
    "\n",
    "#ENFP\n",
    "enfp_posts = []\n",
    "enfp_text = \"\"\n",
    "for post_cluster in df_enfp_posts:\n",
    "    enfp_posts += post_cluster.split('|||')\n",
    "for text in enfp_posts:\n",
    "    enfp_text += \"\".join(text)+\" \"\n",
    "    \n",
    "#ENFJ\n",
    "enfj_posts = []\n",
    "enfj_text = \"\"\n",
    "for post_cluster in df_enfj_posts:\n",
    "    enfj_posts += post_cluster.split('|||')\n",
    "for text in enfj_posts:\n",
    "    enfj_text += \"\".join(text)+\" \"\n",
    "    \n",
    "#ENTP\n",
    "entp_posts = []\n",
    "entp_text = \"\"\n",
    "for post_cluster in df_entp_posts:\n",
    "    entp_posts += post_cluster.split('|||')\n",
    "for text in intp_posts:\n",
    "    entp_text += \"\".join(text)+\" \"\n",
    "    \n",
    "#ENTJ\n",
    "entj_posts = []\n",
    "entj_text = \"\"\n",
    "for post_cluster in df_entj_posts:\n",
    "    entj_posts += post_cluster.split('|||')\n",
    "for text in entj_posts:\n",
    "    entj_text += \"\".join(text)+\" \"\n",
    "    \n",
    "#ESFJ\n",
    "esfj_posts = []\n",
    "esfj_text = \"\"\n",
    "for post_cluster in df_esfj_posts:\n",
    "    esfj_posts += post_cluster.split('|||')\n",
    "for text in esfj_posts:\n",
    "    esfj_text += \"\".join(text)+\" \"\n",
    "\n",
    "#ESFP\n",
    "esfp_posts = []\n",
    "esfp_text = \"\"\n",
    "for post_cluster in df_esfp_posts:\n",
    "    esfp_posts += post_cluster.split('|||')\n",
    "for text in esfp_posts:\n",
    "    esfp_text += \"\".join(text)+\" \"\n",
    "    \n",
    "\n",
    "#ESTJ\n",
    "estj_posts = []\n",
    "estj_text = \"\"\n",
    "for post_cluster in df_estj_posts:\n",
    "    estj_posts += post_cluster.split('|||')\n",
    "for text in estj_posts:\n",
    "    estj_text += \"\".join(text)+\" \"\n",
    "    \n",
    "#ESTP\n",
    "estp_posts = []\n",
    "estp_text = \"\"\n",
    "for post_cluster in df_estp_posts:\n",
    "    estp_posts += post_cluster.split('|||')\n",
    "for text in estp_posts:\n",
    "    estp_text += \"\".join(text)+\" \"\n",
    "\n",
    "#INFP\n",
    "infp_posts = []\n",
    "infp_text = \"\"\n",
    "for post_cluster in df_infp_posts:\n",
    "    infp_posts += post_cluster.split('|||')\n",
    "for text in infp_posts:\n",
    "    infp_text += \"\".join(text)+\" \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-trial",
   "metadata": {},
   "source": [
    "Before we generate the wordclouds, let's preprocess this text a bit..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sudden-qatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RedditScrape import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "skilled-basement",
   "metadata": {},
   "outputs": [],
   "source": [
    "infj_text = preprocess(infj_text)\n",
    "intp_test = preprocess(intp_text)\n",
    "intj_text = preprocess(intj_text)\n",
    "isfp_text = preprocess(isfp_text)\n",
    "isfj_text = preprocess(isfj_text)\n",
    "istp_text = preprocess(istp_text)\n",
    "istj_text = preprocess(istj_text)\n",
    "infp_text = preprocess(infp_text)\n",
    "enfj_text = preprocess(enfj_text)\n",
    "entp_text = preprocess(entp_text)\n",
    "entj_text = preprocess(entj_text)\n",
    "esfp_text = preprocess(esfp_text)\n",
    "esfj_text = preprocess(esfj_text)\n",
    "estp_text = preprocess(estp_text)\n",
    "estj_text = preprocess(estj_text)\n",
    "enfp_text = preprocess(enfp_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-christmas",
   "metadata": {},
   "source": [
    "Now time to make the wordclouds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-tractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_infj = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(infj_text)\n",
    "wc_intp = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(intp_text)\n",
    "wc_intj = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(intj_text)\n",
    "wc_isfp = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(isfp_text)\n",
    "wc_isfj = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(isfj_text)\n",
    "wc_istp = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(istp_text)\n",
    "wc_istj = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(istj_text)\n",
    "wc_infp = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(infp_text)\n",
    "wc_enfj = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(enfj_text)\n",
    "wc_enfp = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(enfp_text)\n",
    "wc_entj = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(entj_text)\n",
    "wc_entp = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(entp_text)\n",
    "wc_esfj = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(esfj_text)\n",
    "wc_esfp = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(esfp_text)\n",
    "wc_estp = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(estp_text)\n",
    "wc_estj = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(estj_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-culture",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME: SAVE ALL AS PNG!!!!!\n",
    "\n",
    "wc_infj.to_file('infj.png')\n",
    "wc_intp.to_file('intp.png')\n",
    "wc_intj.to_file('intj.png')\n",
    "wc_isfp.to_file('isfp.png')\n",
    "wc_istp.to_file('istp.png')\n",
    "wc_istj.to_file('istj.png')\n",
    "wc_infp.to_file('infp.png')\n",
    "wc_enfj.to_file('enfj.png')\n",
    "wc_enfp.to_file('enfp.png')\n",
    "wc_entj.to_file('entj.png')\n",
    "wc_esfj.to_file('esfj.png')\n",
    "wc_esfp.to_file('esfp.png')\n",
    "wc_estj.to_file('estj.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-partition",
   "metadata": {},
   "source": [
    "Use this code to show any of the wordclouds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-faith",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTP\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_entp) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-nancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENFJ\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_enfj) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-ceramic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENFP\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_enfp) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-violin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTJ\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_entj) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-regression",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INFJ\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_infj) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-intro",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INFP\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_infp) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-struggle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INTP\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_intp) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-christopher",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INTJ\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_intj) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-consequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ISFJ\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_isfj) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-puppy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ISFP\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_isfp) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ISTJ\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_istj) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-nightlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ISTP\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_istp) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-effect",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESFJ\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_esfj) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-plain",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESTJ\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_estj) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-credits",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESFP\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_esfp) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-horse",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#ESTP\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_estp) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-ladder",
   "metadata": {},
   "source": [
    "I think that this is pretty satisfactory. Now let's try to parametrize the count of these words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "planned-reset",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'infj_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-40b5b6ecf62c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfj_text\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minfp_text\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mintj_text\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mintp_text\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0misfj_text\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0misfp_text\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mistj_text\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mistp_text\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0menfp_text\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0menfj_text\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mentp_text\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mentj_text\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mesfp_text\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mesfj_text\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mestp_text\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mestj_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'infj_text' is not defined"
     ]
    }
   ],
   "source": [
    "words = list(set((infj_text + \" \" + infp_text + \" \" + intj_text + \" \" + intp_text + \" \" + isfj_text + \" \" + isfp_text + \" \" + istj_text + \" \" + istp_text + \" \" + enfp_text + \" \" + enfj_text + \" \" + entp_text + \" \" + entj_text + \" \" + esfp_text + \" \" + esfj_text + \" \" + estp_text + \" \" + estj_text).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-heading",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-aruba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-basement",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "polar-disposal",
   "metadata": {},
   "source": [
    "Let's try to balance out the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "secret-english",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "df_infj_posts_r = resample(df_infj_posts, replace=True, n_samples=300, random_state=1234)\n",
    "df_infp_posts_r = resample(df_infp_posts, replace=True, n_samples=300, random_state=1234)\n",
    "df_intj_posts_r = resample(df_intj_posts, replace=True, n_samples=300, random_state=1234)\n",
    "df_intp_posts_r = resample(df_intp_posts, replace=True, n_samples=300, random_state=1234)\n",
    "df_isfp_posts_r = resample(df_isfp_posts, replace=True, n_samples=300, random_state=1234)\n",
    "df_isfj_posts_r = resample(df_isfj_posts, replace=True, n_samples=300, random_state=1234)\n",
    "df_istp_posts_r = resample(df_istp_posts, replace=True, n_samples=300, random_state=1234)\n",
    "df_istj_posts_r = resample(df_istj_posts, replace=True, n_samples=300, random_state=1234)\n",
    "\n",
    "#finish\n",
    "df_infj_posts_r = resample(df_infj_posts, replace=True, n_samples=300, random_state=1234)\n",
    "df_infj_posts_r = resample(df_infj_posts, replace=True, n_samples=300, random_state=1234)\n",
    "df_infj_posts_r = resample(df_infj_posts, replace=True, n_samples=300, random_state=1234)\n",
    "df_infj_posts_r = resample(df_infj_posts, replace=True, n_samples=300, random_state=1234)\n",
    "df_infj_posts_r = resample(df_infj_posts, replace=True, n_samples=300, random_state=1234)\n",
    "df_infj_posts_r = resample(df_infj_posts, replace=True, n_samples=300, random_state=1234)\n",
    "df_infj_posts_r = resample(df_infj_posts, replace=True, n_samples=300, random_state=1234)\n",
    "df_infj_posts_r = resample(df_infj_posts, replace=True, n_samples=300, random_state=1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "compliant-tokyo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4676    'Hahaha! I was watching it too! What did you f...\n",
       "4190    'Yes. I can carry conversations. Just this und...\n",
       "7807    'Half of it is going straight to charity, anot...\n",
       "6326    'I'm good! Our house was unscathed :D   Yeah, ...\n",
       "7246    'It depends on the group but in the past, with...\n",
       "                              ...                        \n",
       "2825    'Luckily there are some jobs out there where r...\n",
       "4776    'Hiya,  you do strike me as INFJ from some of ...\n",
       "2410    'If I ever feel the urge to vent, I often feel...\n",
       "3269    'Oh look! My handwriting :)  http://i252.photo...\n",
       "1121    'INFJ here but I like gratitude so hi.   I'm t...\n",
       "Name: posts, Length: 300, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finish\n",
    "df_resampled = pd.concat([df_infj_posts_r, df_infp_posts_r, df_intj_posts_r, df_in, ])\n",
    "\n",
    "#Convert this dataframe into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-scout",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
